We would solve contextual bandit problems, using a policy-\/gradient based reinforcement learning. We would evaluate three different greedy strategies\+: Explore-\/\+First, Epsilon-\/\+Decreasing, and Epsilon-\/\+Greedy.

The reinforcement learning code is derived from \href{https://www.oreilly.com/library/view/tensorflow-powerful-predictive/9781789136913/}{\texttt{ Md. Rezaul Karim}} and \href{https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c}{\texttt{ Arthur Juliani}}.\hypertarget{md__r_e_a_d_m_e_autotoc_md1}{}\doxysection{Table of contents}\label{md__r_e_a_d_m_e_autotoc_md1}

\begin{DoxyItemize}
\item \href{\#Requirements}{\texttt{ Requirements}}
\item \href{\#Setup}{\texttt{ Setup}}
\item \href{\#Instructions}{\texttt{ Instructions}}
\end{DoxyItemize}\hypertarget{md__r_e_a_d_m_e_autotoc_md2}{}\doxysection{Requirements}\label{md__r_e_a_d_m_e_autotoc_md2}
This code submission run in the Jupyter Notebook, so you may need to install Jupyter Notebook. Also, the code is in Python so you will need to use few Python\textquotesingle{}s libraries/packages\+: num\+Py, pandas, Tensor\+Flow, and Matplotib.

Note\+: if you are able to run the code in a different software/environment, you can skip the Setup section. You could install num\+Py, pandas, Tensor\+Flow, and Matplotlib with with Pip.\hypertarget{md__r_e_a_d_m_e_autotoc_md3}{}\doxysection{Setup}\label{md__r_e_a_d_m_e_autotoc_md3}
To run this project, install \href{https://jupyter.org/}{\texttt{ Jupyter Notebook}}. If you have Pip installed in your environment, you could run and install it by entering\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\$ pip install jupyter}
\end{DoxyCode}


Once you get your Jupyter Notebook installed in your operating system, you can launch it by entering\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\$ jupyter notebook}
\end{DoxyCode}




It will take you to the Notebook Dashboard. To create a notebook (the notebook is where you run code), you can click the \char`\"{}\+New\char`\"{} drop-\/down button on top-\/right then select \char`\"{}\+Python 3\char`\"{}. Finally, you are in your first notebook then may run code.

num\+Py and Matplotlib are already installed in Jupyter notebook, you only need to install Tensor\+Flow. Using the Anaconda Prompt, it can be done by entering\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\$ conda install -\/c conda-\/forge tensorflow}
\end{DoxyCode}


Note\+: It depends on your system. There are other commands that you can run with conda. Please read more about the \href{https://anaconda.org/conda-forge/tensorflow}{\texttt{ Tensorflow Installment}}.\hypertarget{md__r_e_a_d_m_e_autotoc_md4}{}\doxysection{Instructions}\label{md__r_e_a_d_m_e_autotoc_md4}
At first, you will need to download \href{https://github.com/ck2860/CodeSubmissionS2/blob/master/data/Ads_Optimisation.csv}{\texttt{ Ads Optimisation data}}, which is already provided in the data folder. You will import the data set in the Notebook. The data set is obtained from \href{https://www.kaggle.com/akram24/ads-ctr-optimisation}{\texttt{ Kaggle}}. You will use pandas and numpy as well.

Note\+: For this reinforcement learning, the data set can be used in online advertising to determine which is the best ad to show the user. It does not mean they would work with other bandit problems.


\begin{DoxyCode}{0}
\DoxyCodeLine{import pandas as pd}
\DoxyCodeLine{import numpy as np }
\DoxyCodeLine{dir ='your directory/'}
\DoxyCodeLine{file = 'Ads\_Optimisation.csv'}
\DoxyCodeLine{adsDF = pd.read\_csv(dir+file)}
\DoxyCodeLine{adsDF.head(2)}
\end{DoxyCode}


Then you will need to find the probability of each ad. 10 ads will be split into 2 arrays. You will have two bandits.


\begin{DoxyCode}{0}
\DoxyCodeLine{meansDF = adsDF.mean()}
\DoxyCodeLine{newarr = np.array\_split(meansDF, 2)}
\DoxyCodeLine{data = np.array([newarr[0], newarr[1]])}
\DoxyCodeLine{data = np.negative([newarr[0], newarr[1]])}
\DoxyCodeLine{data}
\end{DoxyCode}


Your output data should look like this\+:



Now, you can compile explore\+First, epsilon\+Decreasing, and e\+Greedy strategies in the repo. Additionally, there are three different learning rates (0.\+001, 0.\+005, and 0.\+005) that you could test and compare the results. Because of the reinforcement learning and strategies, the evaluation results may be different.

Feel free to email me at \href{mailto:ck2860@rit.edu}{\texttt{ ck2860@rit.\+edu}}.

Thanks for reading! 