<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Main Page </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md0"></a>
Contextual Bandit Methods</h1>
<p>We would solve contextual bandit problems, using a policy-gradient based reinforcement learning. We would evaluate seven different epsilon-based strategies. We have Epsilon-Greedy, Epsilon-Decreasing, and five different combinations of Epsilon-Greedy and Epsilon-Decreasing. We tweaked with the epsilon probability and episodes. Five combinations were created!</p>
<p>In the experiment, we have 10,000 episodes and use the learning rate of 0.05. The epsilon is the probability of exploration.</p>
<p><b>Epsilon-Decreasing:</b> The experiment starts with pure exploration and epsilon is decreased to 0% in the end.</p>
<p><b>Epsilon-Greedy:</b> Epsilon is 10%. The probability is fixed.</p>
<p><b>Hybrid#1:</b> Epsilon is decreased from 90% to 10% throughout the experiment.</p>
<p><b>Hybrid#2:</b> Epsilon is decreased from 100% to 10% in the first 5,000 episodes and keeps as 10% for the rest of the experiment.</p>
<p><b>Hybrid#3:</b> Epsilon is decreased from 90% to 10% in the first 5,000 episodes and keeps as 10% for the rest of the experiment.</p>
<p><b>Hybrid#4:</b> Epsilon is decreased from 100% to 10% in the first 2,500 episodes and keeps as 10% for the rest of the experiment.</p>
<p><b>Hybrid#5:</b> Epsilon is decreased from 90% to 10% in the first 2,500 episodes and keeps as 10% for the rest of the experiment.</p>
<p>All of the strategies are in the evaluation code. The reinforcement learning code is derived from <a href="https://www.oreilly.com/library/view/tensorflow-powerful-predictive/9781789136913/">Md. Rezaul Karim</a> and <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c">Arthur Juliani</a>.</p>
<h2><a class="anchor" id="autotoc_md1"></a>
Table of contents</h2>
<ul>
<li>Requirements</li>
<li>Setup</li>
<li>Instructions</li>
</ul>
<h2><a class="anchor" id="autotoc_md2"></a>
Requirements</h2>
<p>Since all the code are written in Python, you should have or download <a href="https://www.python.org/downloads/">Python</a>.</p>
<p>You will need to have a couple of Python's libraries/packages: numPy, pandas, Tensorflow, matplotlibpyplot, scipy.stats, and atsmodels.stats.multicomp. Most of the packages are pre-installed in Anaconda.</p>
<p>You will be able to run the programs in Anaconda prompt.</p>
<p><em>Note: if you are able to run the programs or have those packages installed in a different software or environment. Make sure your version of tensorflow is 1.14. You can skip the Setup section or (You could install the packages with Pip.)</em></p>
<h2><a class="anchor" id="autotoc_md3"></a>
Setup</h2>
<p>You will be installing <a href="http://anaconda.com/downloads">Anaconda</a> and required packages for this project. Please download the right version for your system/computer.</p>
<p>Recall that numPy, pandas, matplotlibpyplot, and stats packages are already installed in Anaconda, you only need to install TensorFlow 1.14. Using the Anaconda Prompt, it can be done by entering:</p>
<div class="fragment"><div class="line">conda install -c conda-forge tensorflow=1.14</div>
</div><!-- fragment --><p>Note: It depends on your system. There are other commands that you can run with conda. Please read more about <a href="https://anaconda.org/conda-forge/tensorflow">Tensorflow Installment</a>.</p>
<h2><a class="anchor" id="autotoc_md4"></a>
Instructions</h2>
<p>There are two options that you could download the whole code.</p><ol type="1">
<li>Clicking "Code" green button and "download ZIP" on the top right hand on the Git repository page.</li>
<li>Cloning the Git repository by performing git clone.</li>
</ol>
<p>Once you have the files in your system/computer, you should see there are three different datasets: <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/data/Ads_Optimisation.csv">Ads Optimisation</a>, <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/data/MeanRewardsResult.csv">Mean Rewards Results</a>, and <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/data/TukeyData.csv">Tukey Data</a>, which are already provided in the data folder. You will be using them for this code project so make sure they are in the right folder &ndash; data. When you run the python codes, they should be able to find the datasets in the data folder.</p>
<p>The Ads Optimisation data set is obtained from <a href="https://www.kaggle.com/akram24/ads-ctr-optimisation">Kaggle</a>. You will be using this for reinforcement learning and evaluations.</p>
<p>Both Mean Rewards Result and Tukey datasets are the results from the evaluations with 20 random seeds (#1-#20). The Mean Rewards Result data is used for T-tests and ANOVA. Lastly, the Tukey Data is used for Tukey Test.</p>
<p><em>Please make sure they all are in the same directory so the codes would be able to recognize the data sets from the data folder. For this reinforcement learning, the data set can be used in online advertising to determine which is the best ad to show the user. It does not mean they would work with other bandit problems.</em></p>
<p>We have classes in the folder that would be utilized in the running codes. <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/ContextualBandit.py">ContextualBandit</a>, <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/ContextualBanditAgent.py">ContextualBanditAgent</a>, and <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/GreedyStrategies.py">GreedyStrategies</a> are for the evaluations. It would set contextual bandits up, run reinforcement learning, and use greedy-based strategies. Finally, <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/Plot.py">Plot</a> is programmed to plot evaluation results.</p>
<p>You will be able to see five Pythons codes in the file, you could run them if you like! The figures for evaluations and Tukey should be in pop-up windows. For evaluations, there are three Python program that you can run: <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/1trial.py">1trial</a>, <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/10trials.py">10trials</a>, and <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/20trials.py">20trials</a>. 1trial code runs one trial of evaluation. 10trials code runs 10 trials of evaluation. Then 20trials code runs 20 trials of evaluation. You can evaluate by comparing strategies's mean rewards. Last two Python programs: <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/StatsTests.py">StatsTests</a> and <a href="https://github.com/ck2860/MidtermCode-CondyKan/blob/master/Tukey.py">Tukey</a> are used for statistical analysis. We analyze the results from the 20 trials evaluations. You can perform T-tests and ANOVA by running StatsTest code. After narrowing them down and performing a Tukey Pos Hoc test, you may see the differences occurred between greedy-based strategies by compiling Tukey file. Please read <a href="https://ck2860.github.io/MidtermCode-CondyKan/">documentation</a> for more details.</p>
<p>Please open your Anaconda Prompt and go to the directory where you downloaded the project code.</p>
<p>If you want to run one trial of greedy-based strategies of your chosen random seed. For example, if you want to run it with a random seed of 5 (it may take up to 2 minutes), you can compile it by: </p><div class="fragment"><div class="line">python 1trial.py 5</div>
</div><!-- fragment --><p><em>Note that if you use a different random seed for 1trial file, the evaluation results may be different due to reinforcement learning and strategies.</em></p>
<p>If you want to run 10 trials of greedy-based strategies of your chosen random seed (it may take up to 10 minutes), you should compile it by: </p><div class="fragment"><div class="line">python 10trials.py</div>
</div><!-- fragment --><p>If you want to run 20 trials of greedy-based strategies of your chosen random seed (it may take up to 20 minutes), your command line should be: </p><div class="fragment"><div class="line">python 20trials.py</div>
</div><!-- fragment --><p>For t-tests and ANOVA, you would want to run by:</p>
<div class="fragment"><div class="line">python StatsTests.py</div>
</div><!-- fragment --><p>Lastly, you can run a Tukey Test by: </p><div class="fragment"><div class="line">python Tukey.py</div>
</div><!-- fragment --><p>If you have any questions, please feel free to email me at <a href="#" onclick="location.href='mai'+'lto:'+'ck2'+'86'+'0@r'+'it'+'.ed'+'u'; return false;">ck286<span style="display: none;">.nosp@m.</span>0@ri<span style="display: none;">.nosp@m.</span>t.edu</a>.</p>
<p>Thanks for reading! </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
